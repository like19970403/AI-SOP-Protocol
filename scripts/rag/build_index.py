#!/usr/bin/env python3
"""
RAG Index Builder
å»ºç«‹æœ¬åœ°å‘é‡çŸ¥è­˜åº«ï¼Œå°‡ docs/ èˆ‡ profiles/ çš„æ–‡ä»¶å‘é‡åŒ–å¾Œå­˜å…¥ ChromaDB
"""
import argparse
import glob
import os
import sys

def build_index(sources: list[str], output: str, model: str):
    try:
        import chromadb
        from sentence_transformers import SentenceTransformer
    except ImportError:
        print("âŒ ç¼ºå°‘ä¾è³´ï¼Œè«‹åŸ·è¡Œï¼špip install chromadb sentence-transformers")
        sys.exit(1)

    print(f"ğŸ” è¼‰å…¥åµŒå…¥æ¨¡å‹ï¼š{model}")
    embedder = SentenceTransformer(model)

    client = chromadb.PersistentClient(path=output)
    
    # é‡å»ºé›†åˆ
    try:
        client.delete_collection("asp_knowledge")
    except Exception:
        pass
    collection = client.create_collection("asp_knowledge")

    docs, metas, ids = [], [], []
    doc_count = 0

    for source_dir in sources:
        for filepath in glob.glob(f"{source_dir}/**/*.md", recursive=True):
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read().strip()
            if not content:
                continue
            
            # åˆ†å¡Šï¼šæ¯ 500 å­—ä¸€å¡Šï¼Œoverlap 100 å­—
            chunks = chunk_text(content, chunk_size=500, overlap=100)
            for i, chunk in enumerate(chunks):
                doc_id = f"{filepath}::{i}"
                docs.append(chunk)
                metas.append({"source": filepath, "chunk": i})
                ids.append(doc_id)
                doc_count += 1

    if not docs:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
        return

    print(f"ğŸ“š å‘é‡åŒ– {doc_count} å€‹æ–‡ä»¶ç‰‡æ®µ...")
    
    # æ‰¹æ¬¡è™•ç†é¿å… OOM
    batch_size = 100
    for i in range(0, len(docs), batch_size):
        batch_docs = docs[i:i+batch_size]
        batch_metas = metas[i:i+batch_size]
        batch_ids = ids[i:i+batch_size]
        embeddings = embedder.encode(batch_docs).tolist()
        collection.add(
            documents=batch_docs,
            embeddings=embeddings,
            metadatas=batch_metas,
            ids=batch_ids,
        )
        print(f"  é€²åº¦ï¼š{min(i+batch_size, len(docs))}/{len(docs)}", end="\r")

    print(f"\nâœ… RAG ç´¢å¼•å®Œæˆï¼š{doc_count} å€‹ç‰‡æ®µï¼Œå„²å­˜æ–¼ {output}")


def chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> list[str]:
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = min(start + chunk_size, len(words))
        chunks.append(" ".join(words[start:end]))
        start += chunk_size - overlap
    return chunks


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--source", action="append", required=True)
    parser.add_argument("--output", required=True)
    parser.add_argument("--model", default="all-MiniLM-L6-v2")
    args = parser.parse_args()
    build_index(args.source, args.output, args.model)
